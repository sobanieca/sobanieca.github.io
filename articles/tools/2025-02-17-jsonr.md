---
title: "jsonr - interact with JSON API's"
excerpt: "Do you want to efficiently interact with JSON API's within terminal?"
---

Few years ago, after another Postman update that resulted in much slower startup
and overload of features in UI I've decided to search for some alternative. I've
discovered that some of my team mates are using http files and proper extension
within Visual Studio Code. Since I was already in transition period to move
entirely into terminal workflow, I knew I need to find similar tool that has
proper CLI. `curl` was way too complex for me and didn't support http files. The
idea of having repository with http requests to serve as collection was very
attractive. Since I couldn't find proper tool I've decided to build it on my
own.

One may say that it doesn't make any sense especially in AI era where we can interact with API's with natural language. I don't agree, from my experience -
there is always some need interact with API's and there is always need to have specific structure of the request. In this case, collection of http files sounds
like best solution. So why not to simply ask AI to translate http file it into `curl`? The answer is simple - leaking secrets. What I find optimal solution is
to have a strict collection of http files, then some environments defined and simple tool that will merge both and make API request ensuring that no secrets
leak to the LLM.

Leaking secrets is a problem also when interacting with API (via terminal, in GUI it's not a problem since most apps mask it already) during pair-programming sessions or some presentations.

I present to you `jsonr` - the CLI tool that allows you to interact with JSON
API's - https://github.com/sobanieca/jsonr

What's so special about it?

- It's lightweight and simple to use. You only need few minutes to learn about
  it's features. Also, if you use AI agents provide them output from
  `jsonr --help` and they should easily build proper command/http file for you
- It has built-in `secrets` handling so your API keys won't leak whenever you
  ask LLM to make some API request for you or you screen share.
- It encourages you you to use http files. That way, one can keep various
  requests next to the proper API git repository. It allows to share requests
  amongst other developers
- Once requests are properly setup, it's very easy to switch environments and
  with minimal effort make requests to PROD
- It is powered by Deno which means `jsonr` has runtime capabilities built-in
  (!). You can write simple Javascript scripts on top of existing http files and
  chain various requests.
- You can write simple smoke tests with it

It doesn't aim to solve all possible problems out there. It's supposed to handle most (many?) of the common issues when manually interacting with API's. That's why it
is and will remain as simple to use as possible.

Let me provide you an example on how this tool may improve your flow. Let's imagine that you're working on a to-do list app. From time to time you need to test
something on your sandbox environment. Instead of clicking through GUI, you can create a new list:

```create-list.http
POST @@baseUrl@@/lists

{
    "name": "my-list"
}
```

Then you can populate it with some todo items:

```create-todos.http
POST @@baseUrl@@@/lists/@@listId@@/todos

[
  {
    "content": "Buy milk",
  },
  {
    "content": "Buy eggs"
  }
]
```

If you would try to run any of this files with `jsonr` you would notice:

![No variables](./images/jsonr-no-variables.jpg)

`jsonr` detects that within `http` file there is some `@@variable@@` and expects that you provide it. This is to ensure that you won't accidentally `PUT/POST`
some undesired variables.

Now we need to run `jsonr config --init` to initialize configuration file and adjust it according to our needs:

```jsonr-config.json
{
  "environments": {
    "dev": {
      "inputVariables": {
        "baseUrl": "http://localhost",
      },
      "secrets": "~/.secret/dev-secrets.json"
    },
  },

  "defaults": {
    "headers": {
      "X-Request-Source": "jsonr-cli"
    }
  }
}
```

Summary:

Sure, it may take some time to setup all http files and environments. But it's one time job that pays off in future when you need to repeat acting with some
API's. If you're like me and want/already use terminal for your daily professional work I suggest you to give it a try.

> NOTE: I'm not aware about any specifc HTTP file specification that would enforce specific syntax for environment variables or comments. That's why `jsonr`
> comes with it's own interpreation using `@@variable@@` and `# comment` syntax. In future this may change to match future specification.


You're working on

1. The "Golden State" Environment Reset When developing or QA-ing, you often
   need to get a database back to a known "clean" state (e.g., one admin user,
   one empty organization, and one specific subscription).
   - The Workflow: A reset-env.http file containing a sequence of requests:
     DELETE /test-data, POST /seed-admin, POST /create-org.
     - Why it's efficient: Instead of writing a complex bash script or manually
       clicking "Delete" on 50 items in a UI, you just open the file and run the
       requests in order. Itâ€™s a "push-button" solution that stays in your git
       repo, so the whole team has the same "clean state" tool.

              2. Post-Deployment "Smoke Testing"
                Immediately after a deploy to Staging or Production, you need to verify that the "critical path" still works (e.g., Can a user still login?
                Does the search API return results? Is the health-check green?).
                   * The Workflow: A smoke-test.http file with the 5 most important API calls.
                      * Why it's efficient: It takes 10 seconds to run. It bypasses browser caching and UI lag. Most importantly, by using Environment
                        Variables, you can use the exact same file to test localhost, staging.api.com, and api.com just by
                             switching the target environment in your editor.


                               3. The "Emergency Toggle" (Maintenance Mode)
                                 Sometimes you need a way to instantly disable a specific feature or put the site into maintenance mode if a third-party
                                 provider (like a payment gateway or image host) goes down.
                                    * The Workflow: A toggles.http file with requests like PUT /api/v1/config/maintenance-mode with {"enabled": true}.
                                       * Why it's efficient: When there is an emergency, you don't want to be "searching" for the right curl command or
                                         waiting for a slow Admin Dashboard to load. Having a pre-configured, tested request ready to go in your IDE is the
                                         fastest
                                              way to respond to an incident.


                                                4. Monthly "Cleanup" or "Reporting" Nudges
                                                  Some internal systems might not have fully automated "crons" for things like "Close all inactive tickets
                                                  from last month" or "Trigger monthly billing recalculation."
                                                     * The Workflow: A monthly-ops.http file.
                                                        * Why it's efficient: It acts as a Checklist + Tool combined. Opening the file reminds you what
                                                          needs to be done, and the code to do it is right there. It prevents "human error" typos in the
                                                          JSON payload that would happen if you tried
                                                               to do it manually via a CLI every month.


                                                                 5. API Key / Secret Rotation "Sync"
                                                                   When you rotate a service-to-service API key (e.g., your backend's key for a search
                                                                   engine), you might need to tell your secondary "Analytics" service to reload its
                                                                   configuration to pick up the new key.
                                                                      * The Workflow: POST /admin/reload-config with the new metadata.
                                                                         * Why it's efficient: It ensures the "handover" between services is documented.
                                                                           The .http file serves as a "Standard Operating Procedure" (SOP) that is actually
                                                                           executable code.
